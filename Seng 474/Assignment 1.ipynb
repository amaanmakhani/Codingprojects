{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data Sets\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set #1\n",
    "f = open(\".\\Private-Data\\Seng 474\\cleaned_processed.cleveland.data.txt\")\n",
    "data = np.loadtxt(f, delimiter= \",\")\n",
    "# select columns 1 till class column\n",
    "X1 = data[:, 0:-1]\n",
    "Y1 = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set #2\n",
    "f = open(\".\\Private-Data\\Seng 474\\Breast_Cancer.csv\")\n",
    "features = f.readline().split(\",\")\n",
    "features[-1] = features[-1].strip(\"\\n\")\n",
    "data = np.loadtxt(f, delimiter= \",\")\n",
    "# select columns 1 till end and seperate class column\n",
    "X2 = data[:, 1:]\n",
    "Y2 = data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc functions\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(pred, test):\n",
    "    print(\"\\n-----------------------------------Metrics-----------------------------------\")\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(test, pred))\n",
    "    print('           Accuracy:',accuracy_score(test, pred))\n",
    "    print(\"\\n-----------------------------------Report------------------------------------\")\n",
    "    print(classification_report(test,pred))\n",
    "    return accuracy_score(test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Decision Trees\n",
    "--\n",
    "Change the following parameters:<br> \n",
    "    * Number of features\n",
    "    * Tree depth\n",
    "    * Split criterion\n",
    "    * Validation size\n",
    "    * Number of trees\n",
    "    * Pruning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeDecisionTree(X, Y, testSize = 0.2, c = \"gini\", maxDepth = None, minSamplesSplit = 2, maxFeatures = None, minImpurityDecrease = 0.0, rand = 0):\n",
    "    decisionTree = tree.DecisionTreeClassifier(criterion = c, max_depth = maxDepth, min_samples_split = minSamplesSplit, max_features = maxFeatures, random_state = rand, min_impurity_decrease = minImpurityDecrease)\n",
    "    # criterion: The function to measure the quality of a split, information gain(Entropy) or impurity(Gini).\n",
    "    # max_depth: The maximum depth of the tree. \n",
    "    # min_samples_split: The minimum number of samples required to split an internal node.\n",
    "    # max_feature: The number of features to consider when looking for the best split.\n",
    "    # random_state: Controls the randomness of the estimator.\n",
    "    # min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    result = []\n",
    "    print(\"Parameters:\")\n",
    "    print(decisionTree.get_params())\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size = testSize, random_state = rand)\n",
    "    clf = decisionTree.fit(xTrain, yTrain)    \n",
    "    result.append(stats(clf.predict(xTrain), yTrain))\n",
    "    result.append(stats(clf.predict(xTest), yTest))\n",
    "    tree.plot_tree(clf)\n",
    "    print(tree.export_text(clf))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Random Forest\n",
    "--\n",
    "Change the following parameters:\n",
    "    * Number of features\n",
    "    * Tree depth\n",
    "    * Split criterion\n",
    "    * Validation size\n",
    "    * Number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeRandomForest(X, Y, testSize = 0.2, rand = 0, nEst = 100, c = \"gini\",  maxDepth = None, minSamplesSplit = 2, maxFeatures = \"auto\", minImpurityDecrease = 0.0):\n",
    "    classifier = RandomForestClassifier(n_estimators = nEst, criterion = c, max_depth = maxDepth, min_samples_split = minSamplesSplit, max_features = maxFeatures, random_state = rand, min_impurity_decrease = minImpurityDecrease)\n",
    "    # n_estimators: The number of trees in the forest.\n",
    "    # criterion: The function to measure the quality of a split, information gain(Entropy) or impurity(Gini).\n",
    "    # max_depth: The maximum depth of the tree. \n",
    "    # min_samples_split: The minimum number of samples required to split an internal node.  \n",
    "    # max_features: The number of features to consider when looking for the best split. \n",
    "    # random_state: Controls the randomness of the estimator.\n",
    "    # min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    result = []\n",
    "    print(\"Parameters:\")\n",
    "    print(classifier.get_params())\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size = testSize, random_state = rand)\n",
    "    clf = classifier.fit(xTrain, yTrain)    \n",
    "    result.append(stats(clf.predict(xTrain), yTrain))\n",
    "    result.append(stats(clf.predict(xTest), yTest))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Artificial Neural Network\n",
    "--\n",
    "Change the following parameters:<br>\n",
    "    * Learning rate\n",
    "    * Number of hidden layers\n",
    "    * Number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeNeuralNetwork(X, Y, testSize = 0.2, rand = 0, layers = (100,), solverType = 'adam', a = 0.0001, learningRate = 'constant', learningRateVal = 0.001, iterations = 200, earlyStopping = False, validationSet = 0.1, act = 'relu'):\n",
    "    classifier =  MLPClassifier(hidden_layer_sizes = layers , solver = solverType, alpha = a, learning_rate = learningRate, learning_rate_init = learningRateVal , max_iter = iterations, random_state = rand, early_stopping = earlyStopping, validation_fraction = validationSet, activation = act)\n",
    "    # hidden_layer_sizes: The ith element represents the number of neurons in the ith hidden layer.\n",
    "    # solver: ‘lbfgs’ is an optimizer in the family of quasi-Newton methods, and ‘sgd’ refers to stochastic gradient descent.\n",
    "    # alpha: L2 penalty (regularization term) parameter.\n",
    "    # learning_rate: Learning rate schedule for weight updates, and only used when solver='sgd'.\n",
    "    # learning_rate_init: The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
    "    # max_iter: Maximum number of iterations. For sgd this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "    # random_state: Determines random number generation for weights and bias initialization.\n",
    "    # early_stopping: Whether to use early stopping to terminate training when validation score is not improving.\n",
    "    #                 If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving.\n",
    "    #                 Only effective when solver=’sgd’ or ‘adam’.\n",
    "    # validation_fraction: The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.\n",
    "    result = []\n",
    "    print(\"Parameters:\")\n",
    "    print(classifier.get_params())\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size = testSize, random_state = rand)\n",
    "    clf = classifier.fit(xTrain, yTrain)    \n",
    "    result.append(stats(clf.predict(xTrain), yTrain))\n",
    "    result.append(stats(clf.predict(xTest), yTest))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most efficent parameters\n",
    "--\n",
    "**Find the most efficent paramters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestParams(X, Y, pIndex):\n",
    "    if pIndex == 0:\n",
    "        parameters = {'criterion': ['gini', 'entropy'], 'max_depth': np.arange(start=1, stop=10, step=1), 'max_features': np.arange(start=1, stop= len(X), step=2), 'min_samples_split': np.arange(start=1, stop=100, step=5), 'min_impurity_decrease': np.arange(0.0, 0.51, 0.05)}\n",
    "        Classifier = tree.DecisionTreeClassifier()\n",
    "    elif pIndex == 1:\n",
    "        parameters = {'n_estimators': np.arange(start=1, stop=500, step=100),'criterion': ['gini', 'entropy'], 'max_depth': np.arange(start=1, stop=10, step=1), 'max_features': ['auto', 'sqrt', 'log2', None], 'min_samples_split': np.arange(start=1, stop=10, step=1), 'min_impurity_decrease': np.arange(start=0.0, stop=1.0, step=0.1)}\n",
    "        Classifier = RandomForestClassifier()\n",
    "    elif pIndex == 2:\n",
    "        parameters = {'solver': ['lbfgs','sgd'], 'max_iter': np.arange(start=1, stop=800, step=5),'hidden_layer_sizes': np.arange(10, 200, 10), 'learning_rate': ['constant', 'invscaling', 'adaptive']} \n",
    "        Classifier = MLPClassifier()\n",
    "    else:\n",
    "        print(\"Incorrect index\")\n",
    "        return\n",
    "    clf = GridSearchCV(Classifier, parameters, n_jobs=-1)\n",
    "    clf.fit(X,Y)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Graph\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(a, fa, b, fb, name, xvalrange = 100, xAxisTitle = 'Test Size (%)', title = \"Test Size\", digits = 5):\n",
    "    plt.clf()\n",
    "    plt.plot(a, fa, linewidth=3.0, label='Training Data')\n",
    "    plt.plot(b, fb, linewidth=3.0, label='Test Data')\n",
    "    ymax = max(fb)\n",
    "    xpos = fb.index(ymax)\n",
    "    xmax = x[xpos]\n",
    "    plt.annotate('Maximum \\n(' + str(xmax) + ', ' + str(ymax)[0:digits] + ')', xy=(xmax, ymax), xytext=(xmax, ymax -30), arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    plt.xlabel(xAxisTitle)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlim(0, xvalrange)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.title(title + ' vs. Accuracy')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.savefig(name  + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main program -- Decision Trees\n",
    "--\n",
    "**Contains expirements and parameter changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decision tree no specified parameters using default parameters\n",
    "# Change test and training distributed size to view change in accurracy\n",
    "# Expirement 1\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = i/100)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = i/100)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree using entropy instead of gini for the split crierion\n",
    "# Change test and training distributed size to view change in accurracy\n",
    "# Expirement 2\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = i/100, c = 'entropy')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = i/100, c = 'entropy')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying depth\n",
    "# Expirement 3\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying depth and using entropy instead of gini for the split crierion\n",
    "# Expirement 4\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, c = 'entropy', maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, c = 'entropy', maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\",  xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying minimum split samples\n",
    "# Expirement 5\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 8, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, minSamplesSplit = 2 ** i )\n",
    "    x.append(2 ** i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 2 ** 7, xAxisTitle = \"Minimum Split Samples\", title = \"Minimum Split Samples\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1,10, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, minSamplesSplit = 2 ** i )\n",
    "    x.append(2 ** i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 2 ** 9, xAxisTitle = \"Minimum Split Samples\", title = \"Minimum Split Samples\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying minimum split samples and using entropy instead of gini for the split crierion\n",
    "# Expirement 6\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 8, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2,  c = 'entropy', minSamplesSplit = 2 ** i )\n",
    "    x.append(2 ** i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 2 ** 7, xAxisTitle = \"Minimum Split Samples\", title = \"Minimum Split Samples\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1,10, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, c = 'entropy', minSamplesSplit = 2 ** i )\n",
    "    x.append(2 ** i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 2 ** 9, xAxisTitle = \"Minimum Split Samples\", title = \"Minimum Split Samples\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying randomization\n",
    "# Expirement 7\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 31, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, c = 'entropy', rand = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 30, xAxisTitle = \"Randomization\", title = \"Randomization\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 31, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, c = 'entropy', rand = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 30, xAxisTitle = \"Randomization\", title = \"Randomization\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying randomization and using entropy instead of gini for the split crierion\n",
    "# Expirement 8\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 31, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, rand = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 30, xAxisTitle = \"Randomization\", title = \"Randomization\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 31, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, rand = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 30, xAxisTitle = \"Randomization\", title = \"Randomization\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying max features\n",
    "# Expirement 9\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(2, 14, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, maxFeatures = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 13, xAxisTitle = \"Max Features\", title = \"Max Feautures\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(2, 31, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, maxFeatures = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 30, xAxisTitle = \"Max Features\", title = \"Max Feautures\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying max features and using entropy instead of gini for the split crierion\n",
    "# Expirement 10\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(2, 14, 1):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, c = 'entropy',  maxFeatures = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 13, xAxisTitle = \"Max Features\", title = \"Max Feautures\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(2, 31, 1):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, c = 'entropy', maxFeatures = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 30, xAxisTitle = \"Max Features\", title = \"Max Feautures\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying impurity decrease\n",
    "# Expirement 11\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in np.arange(0.0, 0.51, 0.05):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, minImpurityDecrease = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 0.5, xAxisTitle = \"Impurity Decrease\", title = \"Impurity Decrease\", digits = 2)\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in np.arange(0, 0.51, 0.05):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, minImpurityDecrease = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 0.5, xAxisTitle = \"Impurity Decrease\", title = \"Impurity Decrease\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with varying impurity decrease and using entropy instead of gini for the split crierion\n",
    "# Expirement 12\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in np.arange(0.0, 0.51, 0.05):\n",
    "    accurracy = executeDecisionTree(X1, Y1, testSize = 0.2, c = 'entropy', minImpurityDecrease = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 0.5, xAxisTitle = \"Impurity Decrease\", title = \"Impurity Decrease\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in np.arange(0, 0.51, 0.05):\n",
    "    accurracy = executeDecisionTree(X2, Y2, testSize = 0.2, c = 'entropy', minImpurityDecrease = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 0.5, xAxisTitle = \"Impurity Decrease\", title = \"Impurity Decrease\", digits = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main program -- Random Forests\n",
    "--\n",
    "**Contains expirements and parameter changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random forest no specified parameters using default parameters\n",
    "# Change test and training distributed size to view change in accurracy\n",
    "# Expirement 13\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeRandomForest(X1, Y1, testSize = i/100)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeRandomForest(X2, Y2, testSize = i/100)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest using entropy instead of gini for the split crierion\n",
    "# Change test and training distributed size to view change in accurracy\n",
    "# Expirement 14\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeRandomForest(X1, Y1, testSize = i/100, c = 'entropy')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeRandomForest(X2, Y2, testSize = i/100, c = 'entropy')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest with varying number of estimators\n",
    "# Expirement 15\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 1002, 50):\n",
    "    accurracy = executeRandomForest(X1, Y1, nEst = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 1000, xAxisTitle = \"Number of Estimators\", title = \"Number of Estimators\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 1002, 50):\n",
    "    accurracy = executeRandomForest(X2, Y2, nEst = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 1000, xAxisTitle = \"Number of Estimators\", title = \"Number of Estimators\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest with varying number of estimators and entropy instead of gini for the split crierion\n",
    "# Expirement 16\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 1002, 50):\n",
    "    accurracy = executeRandomForest(X1, Y1, nEst = i, c = 'entropy')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 1000, xAxisTitle = \"Number of Estimators\", title = \"Number of Estimators\", digits = 2)\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 1002, 50):\n",
    "    accurracy = executeRandomForest(X2, Y2, nEst = i, c = 'entropy')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 1000, xAxisTitle = \"Number of Estimators\", title = \"Number of Estimators\", digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest with varying depth\n",
    "# Expirement 17\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeRandomForest(X1, Y1, testSize = 0.2, maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeRandomForest(X2, Y2, testSize = 0.2, maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest with varying depth and using entropy instead of gini for the split crierion\n",
    "# Expirement 18\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeRandomForest(X1, Y1, testSize = 0.2, c = 'entropy', maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 9, 1):\n",
    "    accurracy = executeRandomForest(X2, Y2, testSize = 0.2, c = 'entropy', maxDepth = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\",  xvalrange = 9, xAxisTitle = \"Tree Depth\", title = \"Tree Depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main program -- Neural Networks\n",
    "--\n",
    "**Contains expirements and parameter changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with varying test and training distributed size to view change in accurracy\n",
    "# Expirement 19\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, testSize = i/100)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 91, 5):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, testSize = i/100)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with varying number of layers\n",
    "# Expirement 20\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 30, 1):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, layers = tuple([100 for item in range(1,i)]))\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 30, xAxisTitle = \"Number of Layers\", title = \"Number of Layers\")\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(1, 30, 1):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, layers = tuple([100 for item in range(1,i)]))\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 30, xAxisTitle = \"Number of Layers\", title = \"Number of Layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with varying number of interations using sgd as the solver\n",
    "# Expirement 21\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(100, 1501, 50):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, solverType = 'sgd', iterations = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 1500, xAxisTitle = \"Number of Iterations\", title = \"Number of Iterations\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(100, 1501, 50):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, solverType = 'sgd', iterations = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 1500, xAxisTitle = \"Number of Iterations\", title = \"Number of Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with varying number of iterations using Adam as the solver\n",
    "# Expirement 22\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(100, 1501, 50):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, iterations = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 1500, xAxisTitle = \"Number of Iterations\", title = \"Number of Iterations\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(100, 1501, 50):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, iterations = i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 1500, xAxisTitle = \"Number of Iterations\", title = \"Number of Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with varying Alpha value\n",
    "# Expirement 23\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 5, 1):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, a = 1000.0 ** -i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 4, xAxisTitle = \"Alpha (1000.0^(-x))\", title = \"Alpha\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 5, 1):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, a = 1000.0 ** -i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 4, xAxisTitle = \"Alpha (1000.0^(-x))\", title = \"Alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with varying number layer size and ‘logistic’ activation function\n",
    "# Expirement 24\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 301, 10):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, layers = (i,i,), act = 'logistic')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 300, xAxisTitle = \"Number of Hidden Neurons per Layer\", title = \"Number of Hidden Neurons\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 301, 10):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, layers = (i,i,), act = 'logistic')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 300, xAxisTitle = \"Number of Hidden Neurons per Layer\", title = \"Number of Hidden Neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with varying learning rate\n",
    "# Expirement 25\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 5, 1):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, learningRateVal= 1000.0 ** -i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange =  4, xAxisTitle = \"Learning Rate (1000.0^(-x))\", title = \"Learning Rate\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(0, 5, 1):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, learningRateVal= 1000.0 ** -i)\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 4, xAxisTitle = \"Learning Rate (1000.0^(-x))\", title = \"Learning Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network using adaptive learning rate\n",
    "# Expirement 26\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 301, 10):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, learningRate = 'adaptive', layers = (i,i,), solverType = 'sgd')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 300, xAxisTitle = \"Number of Hidden Neurons per Layer\", title = \"Number of Hidden Neurons\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 301, 10):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, learningRate = 'adaptive', layers = (i,i,), solverType = 'sgd')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 300, xAxisTitle = \"Number of Hidden Neurons per Layer\", title = \"Number of Hidden Neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network using invscaling learning rate\n",
    "# Expirement 27\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 301, 10):\n",
    "    accurracy = executeNeuralNetwork(X1, Y1, learningRate = 'invscaling', layers = (i,i,), solverType = 'sgd')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet1\", xvalrange = 300, xAxisTitle = \"Number of Hidden Neurons per Layer\", title = \"Number of Hidden Neurons\")\n",
    "\n",
    "\n",
    "x = []\n",
    "fa = []\n",
    "fb = []\n",
    "for i in range(10, 301, 10):\n",
    "    accurracy = executeNeuralNetwork(X2, Y2, learningRate = 'invscaling', layers = (i,i,), solverType = 'sgd')\n",
    "    x.append(i)\n",
    "    fa.append(accurracy[0] * 100)\n",
    "    fb.append(accurracy[1] * 100)\n",
    "plotGraph(x, fa, x, fb, \"DataSet2\", xvalrange = 300, xAxisTitle = \"Number of Hidden Neurons per Layer\", title = \"Number of Hidden Neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Portion\n",
    "--\n",
    "**Optimize all the model parameters to achieve the best accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best decision tree parameters and score, Dataset 1\")\n",
    "findBestParams(X1, Y1, 0)\n",
    "print(\"Best decision tree parameters and score, Dataset 2\")\n",
    "findBestParams(X2, Y2, 0)\n",
    "print(\"Best random forest parameters and score, Dataset 1\")\n",
    "findBestParams(X1, Y1, 1)\n",
    "print(\"Best random forest parameters and score, Dataset 2\")\n",
    "findBestParams(X2, Y2, 1)\n",
    "print(\"Best neural network parameters and score, Dataset 1\")\n",
    "findBestParams(X1, Y1, 2)\n",
    "print(\"Best neural network parameters and score, Dataset 2\")\n",
    "findBestParams(X2, Y2, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
